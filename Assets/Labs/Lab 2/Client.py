import socket
import json
import pyrealsense2 as rs
import numpy as np
import cv2
import time
import threading

HOST = "10.47.101.196"   # <- æ”¹æˆä½ çš„ Unity Server IP
PORT = 143

latest_msg = {}
id_transformation_matrix = {}   # dict: id -> 4x4 homogeneous matrix H (camera_col -> world_col)
anchor_created = False
aruco_size = 0.16  # ArUco çœŸå¯¦é‚Šé•· (å…¬å°º)ï¼Œè«‹æ”¹æˆä½ å¯¦éš›å€¼

# å­˜æ”¾æœ€è¿‘ä¸€å¹€æ¯å€‹ marker çš„ 4 å€‹è§’é» (camera coords, shape (4,3))
detected_marker_corners = {}  # id -> np.array(4,3)

# æ¥æ”¶ thread
def receive_loop(sock):
    global latest_msg, anchor_created
    while True:
        try:
            data = sock.recv(4096)
            if not data:
                print("Socket closed by server")
                break
            # å‡è¨­æ¯æ¬¡ recv å°±æ˜¯ä¸€å€‹å®Œæ•´çš„ JSONï¼ˆç°¡åŒ–è™•ç†ï¼‰
            text = data.decode("utf-8")
            try:
                msg = json.loads(text)
            except Exception as e:
                print("JSON decode error:", e, "raw:", text)
                continue
            latest_msg = msg
            anchor_created = True
            print("ğŸ“© Received:", msg)
        except Exception as e:
            # ä¸è¦çµ‚æ­¢ threadï¼Œç¨ç­‰å†ç¹¼çºŒ
            # print("receive_loop error:", e)
            time.sleep(0.01)
            continue

def send(sock, msg):
    try:
        data = json.dumps(msg)
        sock.sendall(data.encode("utf-8"))
        print("ğŸ“¤ Sent:", msg)
    except Exception as e:
        print("Send error:", e)

# æ§‹é€  4x4 é½Šæ¬¡çŸ©é™£ Hï¼Œå¾ lstsq å¾—åˆ°çš„ M (4x3)ï¼šæ»¿è¶³ row-vector: P_cam_row @ M = P_world_row
# æˆ‘å€‘è½‰æˆ column-vector convention: H @ P_cam_col = P_world_col
def build_homogeneous_from_lstsq_M(M_4x3):
    # M_4x3: shape (4,3)
    H = np.eye(4, dtype=float)
    # H[:3,:4] such that H @ [x,y,z,1]^T = [P_world; 1]
    H[:3, :4] = M_4x3.T  # M.T is 3x4
    return H

# RealSense init
pipeline = rs.pipeline()
config = rs.config()

pipeline_wrapper = rs.pipeline_wrapper(pipeline)
pipeline_profile = config.resolve(pipeline_wrapper)
device = pipeline_profile.get_device()

found_rgb = False
for s in device.sensors:
    if s.get_info(rs.camera_info.name) == 'RGB Camera':
        found_rgb = True
        break
if not found_rgb:
    print("The demo requires Depth camera with Color sensor")
    exit(0)

config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)

pipeline.start(config)
time.sleep(0.5)  # warmup

# ArUco
arucoDict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_1000)
arucoParams = cv2.aruco.DetectorParameters()
arucoDetector = cv2.aruco.ArucoDetector(arucoDict, arucoParams)

# æ§åˆ¶å›å‚³é »ç‡
last_send_time = 0.0
send_interval = 0.1  # ç§’

with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
    sock.connect((HOST, PORT))
    threading.Thread(target=receive_loop, args=(sock,), daemon=True).start()

    try:
        while True:
            # å– frameï¼ˆtimeout å¯è¦–éœ€è¦èª¿æ•´ï¼‰
            try:
                frames = pipeline.wait_for_frames(timeout_ms=5000)
            except RuntimeError as e:
                print("Frame wait error:", e)
                time.sleep(0.1)
                continue

            depth_frame = frames.get_depth_frame()
            color_frame = frames.get_color_frame()
            if not depth_frame or not color_frame:
                continue

            depth_intrin = depth_frame.profile.as_video_stream_profile().intrinsics
            color_image = np.asanyarray(color_frame.get_data())
            depth_image = np.asanyarray(depth_frame.get_data())

            corners, ids, rejected = arucoDetector.detectMarkers(color_image)
            color_image = cv2.aruco.drawDetectedMarkers(color_image, corners, ids)

            detected_marker_corners.clear()

            if ids is not None and len(ids) > 0:
                for i, mid in enumerate(ids.flatten()):
                    # æ”¶é›†è©² marker çš„ 4 å€‹è§’ corner çš„ 3D camera coords
                    pts_cam = []
                    for p in corners[i][0]:
                        px, py = int(p[0]), int(p[1])
                        # æœ‰æ™‚å€™æ·±åº¦å–ä¸åˆ°ï¼Œéœ€è·³éæˆ–ç”¨å‚™æ´
                        depth = depth_frame.get_distance(px, py)
                        if depth == 0 or np.isnan(depth):
                            # è‹¥æŸè§’å–å¾— depth å¤±æ•—ï¼Œè·³éæ­¤ marker
                            pts_cam = []
                            break
                        X, Y, Z = rs.rs2_deproject_pixel_to_point(depth_intrin, [px, py], depth)
                        pts_cam.append([X, Y, Z])
                    if len(pts_cam) == 4:
                        detected_marker_corners[int(mid)] = np.array(pts_cam, dtype=float)
                    else:
                        # è‹¥ç„¡æ³•å–å¾— 4 é»æ·±åº¦ï¼Œå°±ä¸æ”¾å…¥ dict
                        print(f"Marker {int(mid)}: cannot get 4 corner depths; skip this frame for this marker.")

            # å¦‚æœå·²ç¶“æœ‰ transformation çŸ©é™£ï¼Œå°åµæ¸¬åˆ°çš„ marker ä¸­å¿ƒåšè½‰æ›ä¸¦å›å‚³çµ¦ server
            tnow = time.time()
            if tnow - last_send_time >= send_interval:
                last_send_time = tnow
                for mid, pts_cam in detected_marker_corners.items():
                    if mid in id_transformation_matrix:
                        # ä¸­å¿ƒé» (camera coords)
                        center_cam = np.mean(pts_cam, axis=0)  # shape (3,)
                        # è½‰æˆé½Šæ¬¡ column vector
                        cam_h = np.array([center_cam[0], center_cam[1], center_cam[2], 1.0], dtype=float)
                        H = id_transformation_matrix[mid]  # 4x4
                        world_h = H @ cam_h  # column-vector
                        # è‹¥ last element é 1ï¼Œå‰‡é½Šæ¬¡åŒ–
                        if abs(world_h[3]) > 1e-8:
                            world = world_h[:3] / world_h[3]
                        else:
                            world = world_h[:3]
                        send(sock, {
                            "id": int(mid),
                            "transformed_position": {
                                "x": float(world[0]),
                                "y": float(world[1]),
                                "z": float(world[2])
                            }
                        })

            # è‹¥ Unity è¦å»ºç«‹ anchorï¼ˆanchor_createdï¼‰ï¼Œç”¨ç•¶å‰åµæ¸¬åˆ°çš„ marker 4 å€‹ corner èˆ‡ Unity çµ¦çš„ center è¨ˆç®— transform
            if anchor_created:
                anchor_created = False  # å…ˆé‡ç½®æ——æ¨™ï¼Œé¿å…é‡è¤‡è™•ç†åŒä¸€å‰‡è¨Šæ¯
                msg = latest_msg.copy()
                try:
                    mid = msg.get("id")
                    corners_from_unity = msg.get("ArUcoCornerPos")  # list of 4 dicts

                    print ("mid: " + str(mid))
                    print ("corners_from_unity: " + str(corners_from_unity))
                    if mid is None or corners_from_unity is None:
                        print("Anchor message missing id or position")
                    else:
                        # å¿…é ˆåœ¨æœ¬ frame æˆ–æœ€è¿‘æœ‰åµæ¸¬åˆ°è©² marker çš„ 4 å€‹è§’
                        if mid not in detected_marker_corners:
                            print(f"Marker {mid} not detected (no 4 corners) â€” wait next frame")
                        else:
                            sensor_points = detected_marker_corners[mid]  # (4,3)
                            target_points = np.array([[c["x"], c["y"], c["z"]] for c in corners_from_unity], dtype=float)  # 4x3

                            # è§£ A @ M = B ï¼ŒA: (4x4) sensor points hom, B: (4x3) target_points
                            A = np.hstack([sensor_points, np.ones((4, 1), dtype=float)])  # 4x4
                            B = target_points  # 4x3

                            M_4x3, residuals, rank, s = np.linalg.lstsq(A, B, rcond=None)
                            # build 4x4 homogeneous matrix H such that H @ [x,y,z,1]^T = [xw,yw,zw,1]^T
                            H = build_homogeneous_from_lstsq_M(M_4x3)
                            id_transformation_matrix[mid] = H
                            print(f"ğŸ§® Computed transformation matrix for id {mid}:\n{H}")
                except Exception as e:
                    print("Error handling anchor msg:", e)

            # å¯é¸çš„å½±åƒå„²å­˜ï¼ˆå–ä»£è¦–è¦ºåŒ–é¡¯ç¤ºï¼‰
            # å¦‚æœéœ€è¦æŸ¥çœ‹å½±åƒï¼Œå¯ä»¥å®šæœŸå„²å­˜åˆ°æª”æ¡ˆ
            save_images = False  # è¨­ç‚º True ä¾†å•Ÿç”¨å½±åƒå„²å­˜
            if save_images and tnow - last_send_time >= 5.0:  # æ¯5ç§’å„²å­˜ä¸€æ¬¡
                depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)
                if depth_colormap.shape != color_image.shape:
                    color_vis = cv2.resize(color_image, (depth_colormap.shape[1], depth_colormap.shape[0]))
                else:
                    color_vis = color_image
                combined_image = np.hstack((color_vis, depth_colormap))
                timestamp = int(time.time())
                cv2.imwrite(f"realsense_output_{timestamp}.jpg", combined_image)
                print(f"ğŸ’¾ Saved image: realsense_output_{timestamp}.jpg")
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦é€€å‡ºï¼ˆå¯ä»¥ç”¨ Ctrl+Cï¼‰
            time.sleep(0.01)  # å°å»¶é²é¿å…éåº¦ä½”ç”¨CPU

    finally:
        pipeline.stop()
        print("ğŸ”Œ Pipeline stopped and resources cleaned up")
